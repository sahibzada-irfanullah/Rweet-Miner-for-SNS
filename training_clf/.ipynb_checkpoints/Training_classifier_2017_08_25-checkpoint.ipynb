{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running 1 out of 2 fold(s):for Unigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-251\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "\n",
      "Running 2 out of 2 fold(s):for Unigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-251\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "Results saved to the worksheet\n",
      "\n",
      "\n",
      "Running 1 out of 2 fold(s):for UniAndBigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-660\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "\n",
      "Running 2 out of 2 fold(s):for UniAndBigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-660\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "Results saved to the worksheet\n",
      "\n",
      "\n",
      "Running 1 out of 2 fold(s):for UniBiAndTrigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-1096\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "\n",
      "Running 2 out of 2 fold(s):for UniBiAndTrigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-1096\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "Results saved to the worksheet\n",
      "\n",
      "\n",
      "Running 1 out of 2 fold(s):for Bigrams_CountVect_minDF-1_maxDF-1.0_WoRul_Freq-409\n",
      "[==================================================] [100.0%] [eta:0:00:00]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')#replace ignore with default for enabling the warning\n",
    "import gc\n",
    "import os\n",
    "import xlsxwriter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from utilities.my_XL_Cls import XL_Results_writing\n",
    "\n",
    "class Training_Classifiers:\n",
    "  \n",
    "  def cal_accuracy(self, y_test, y_pred_class, clf_name):\n",
    "    # conf_metrics = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "    acc = metrics.accuracy_score(y_test, y_pred_class) * 100\n",
    "    prec = metrics.precision_score(y_test, y_pred_class, average='weighted') * 100\n",
    "    recal = metrics.recall_score(y_test, y_pred_class, average=\"weighted\") * 100\n",
    "    f1 = metrics.f1_score(y_test, y_pred_class, average='weighted') * 100\n",
    "    # if clf_name == \"Random Forest\":\n",
    "#     print([\"{0:.2f}\".format(acc), \"{0:.2f}\".format(prec), \"{0:.2f}\".format(recal), \"{0:.2f}\".format(f1)])\n",
    "    # saveData_cross_check(y_test, y_pred_class, clf_name)\n",
    "    return [\"{0:.2f}\".format(acc), \"{0:.2f}\".format(prec), \"{0:.2f}\".format(recal), \"{0:.2f}\".format(f1)]\n",
    "\n",
    "\n",
    "  def apply_SVM_KF(self, X_train_dtm, X_test_dtm, y_train):\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = clf.predict(X_test_dtm)\n",
    "    # print(\"SVM completed\")\n",
    "    return y_pred_class\n",
    "\n",
    "\n",
    "  def apply_Logistic_KF(self, X_train_dtm, X_test_dtm, y_train):\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = logreg.predict(X_test_dtm)\n",
    "    # print(\"LR completed\")\n",
    "    return y_pred_class\n",
    "\n",
    "  def apply_RandomForest_KF(self, X_train_dtm, X_test_dtm, y_train):\n",
    "    # clf_randomForest = RandomForestClassifier(n_estimators=382, criterion='entropy', max_features=116, max_depth=33, min_samples_split=5, min_samples_leaf=1 )\n",
    "    clf_randomForest = RandomForestClassifier()\n",
    "    clf_randomForest.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = clf_randomForest.predict(X_test_dtm)\n",
    "    # print(\"RF completed\")\n",
    "    return y_pred_class\n",
    "\n",
    "  def apply_NaiveBayes_KF(self, X_train_dtm, X_test_dtm, y_train):\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    # path = 'Classifiers/' + 'NB' + '.pkl'\n",
    "    # with open(path, 'wb') as f:\n",
    "    #   pickle.dump(nb, f)\n",
    "    # print(\"NB completed\")\n",
    "    return y_pred_class\n",
    "\n",
    "  def apply_GradientBoostingClf_KF(self, X_train_dtm, X_test_dtm, y_train):\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = gb.predict(X_test_dtm)\n",
    "    # print(\"GB completed\")\n",
    "    return y_pred_class\n",
    "\n",
    "  def apply_MLP_KF(self, X_train_dtm, X_test_dtm, y_train):\n",
    "    clf = MLPClassifier(solver='sgd')\n",
    "    clf.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = clf.predict(X_test_dtm)\n",
    "    # print(\"MLP completed\")\n",
    "    return y_pred_class\n",
    "  # unit test OKKKK\n",
    "\n",
    "  def ls_ToDf(self ,ls1 , ls2, ls3, ls4, ls5, ls6):\n",
    "    '''\n",
    "    -convert list type into DataFrame and add columns names for creating labelled table of dataframe type\n",
    "    + takes six lists of arguments and convert it Dataframe row wise order\n",
    "    '''\n",
    "    ls_digit1 = list(map(float, ls1))\n",
    "    ls_digit2 = list(map(float, ls2))\n",
    "    ls_digit3 = list(map(float, ls3))\n",
    "    ls_digit4 = list(map(float, ls4))\n",
    "    ls_digit5 = list(map(float, ls5))\n",
    "    ls_digit6 = list(map(float, ls6))\n",
    "    res = list()\n",
    "    res.append(ls_digit1)\n",
    "    res.append(ls_digit2)\n",
    "    res.append(ls_digit3)\n",
    "    res.append(ls_digit4)\n",
    "    res.append(ls_digit5)\n",
    "    res.append(ls_digit6)\n",
    "    df = pd.DataFrame(res, columns=['Accuracy', 'Precision', 'Recall', 'F1-Measure'])\n",
    "    df['Classifier']=['Naive Bayes', 'Logisitic Regression', 'SVM', 'Random Forest', 'Gradient Boosting', 'NLP']\n",
    "    df = df[['Classifier','Accuracy', 'Precision', 'Recall', 'F1-Measure']]\n",
    "    df.set_index('Classifier', inplace = True)\n",
    "    return df\n",
    "  \n",
    "  def stratified_cv(self, X, y, uniSpecs, n_splits=5, shuffle=True):\n",
    "    stratified_k_fold = StratifiedKFold(n_splits=n_splits, shuffle=shuffle)\n",
    "    X_train_dtm = X\n",
    "    X_train_dtm = X_train_dtm.toarray()\n",
    "    y_pred_NB = y.copy()\n",
    "    y_pred_LogReg = y.copy()\n",
    "    y_pred_RForest= y.copy()\n",
    "    y_pred_gbClf = y.copy()\n",
    "    y_pred_SVM = y.copy()\n",
    "    y_pred_MLP = y.copy() \n",
    "    num_Clf = 6 #this is equal to number of classifiers used for testing\n",
    "    iteration = 0\n",
    "    for train_index, test_index in stratified_k_fold.split(X_train_dtm, y):\n",
    "      message = \"\\n\\nRunning \"+str(iteration+1)+\" out of \"+str(n_splits)+' fold(s):'+ \"for \"+ uniSpecs\n",
    "      pBar = My_progressBar(message,num_Clf)\n",
    "      start_time = time.time()\n",
    "      X_train, X_test = X_train_dtm[train_index], X_train_dtm[test_index]\n",
    "      y_train = y[train_index]\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      y_pred_NB[test_index] = self.apply_NaiveBayes_KF(X_train, X_test, y_train)\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      y_pred_LogReg[test_index] = self.apply_Logistic_KF(X_train, X_test, y_train)\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      y_pred_SVM[test_index] = self.apply_SVM_KF(X_train, X_test, y_train)\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      y_pred_RForest[test_index] = self.apply_RandomForest_KF(X_train, X_test, y_train)\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      y_pred_gbClf[test_index] = self.apply_GradientBoostingClf_KF(X_train, X_test, y_train)\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      y_pred_MLP[test_index] = self.apply_MLP_KF(X_train, X_test, y_train)\n",
    "      iteration =  iteration + 1\n",
    "      pBar.call_to_progress(start_time)\n",
    "      start_time = time.time()\n",
    "      pBar = \"\"\n",
    "      gc.collect()\n",
    "  #     print(str(iteration) + \" fold completed\")\n",
    "    # temp = pd.DataFrame([y, y_pred_NB], columns=['actual', 'predicted'])\n",
    "    # print(temp.head())\n",
    "    score_NB = self.cal_accuracy(y, y_pred_NB, \"NB\")\n",
    "    score_LogReg = self.cal_accuracy(y, y_pred_LogReg, \"Logisict Regression\")\n",
    "    score_SVM = self.cal_accuracy(y, y_pred_SVM, \"SVM\")\n",
    "    score_RForest = self.cal_accuracy(y, y_pred_RForest, \"Random Forest\")\n",
    "    score_GBClf = self.cal_accuracy(y, y_pred_gbClf, \"GB Classifier\")\n",
    "    score_MLP = self.cal_accuracy(y, y_pred_MLP, \"MLP Classifier\")\n",
    "    # return score_NB, X_train_dtm.shape\n",
    "  #   dim[1] = X_train_dtm.shape\n",
    "    df = self.ls_ToDf(score_NB, score_LogReg, score_SVM, score_RForest, score_GBClf, score_MLP)\n",
    "  #   xl.save_resultsToExcel(df1, label, label, dim[1])\n",
    "    # return score_NB, score_LogReg, score_SVM, score_RForest, score_GBClf ,score_MLP, X_train_dtm.shape\n",
    "    return df\n",
    "    \n",
    "  def list_str(self, list1):\n",
    "    '''Def: convert list to string by joing list element using _ \n",
    "    Args: pass python list as an argument\n",
    "    Ret: retrun resultant string'''\n",
    "    return '_'.join(list1)\n",
    "\n",
    "  def check_file(self, fname, path = '../files/'):\n",
    "    created_at = datetime.datetime.now().strftime(\"%y-%m-%d_%H%M-%S\")\n",
    "    temp = path + created_at +'_'+ fname \n",
    "    path =temp + '.xlsx'\n",
    "    while(os.path.isfile(path) == True):\n",
    "      old = path\n",
    "      temp +=' copy'\n",
    "      path =temp +'.xlsx'\n",
    "    workbook = xlsxwriter.Workbook(path)\n",
    "    workbook.close() \n",
    "    return path\n",
    "  \n",
    "  def extractInfo_specs(self, st, req):\n",
    "    '''Def: fetch required info from list\n",
    "    Args: pass list and req arguments where req contained name of the required info to be fetched from list\n",
    "    Ret: return required info'''\n",
    "    st = st.split('_')\n",
    "    if req == 'sheet_name':\n",
    "      info = st[0]\n",
    "    elif req == 'file_name':\n",
    "      info = self.list_str(st[1:])\n",
    "    elif req == 'freq':\n",
    "      info = st[-1].split('-')[1]\n",
    "    return info\n",
    "\n",
    "  \n",
    "  def training_clfs(self, encap_res, y, k_fold = 2):\n",
    "    for key,value in encap_res.items():\n",
    "      if key == 'unigrams':\n",
    "        uniSpecs = value['specs']\n",
    "        uniTrainDtm = value['dtm']\n",
    "      elif key == 'uniBigrams':\n",
    "        uniBiSpecs = value['specs']\n",
    "        uniBiTrainDtm = value['dtm']\n",
    "      elif key == 'uniBiTrigrams':\n",
    "        uniBiTriSpecs = value['specs']\n",
    "        uniBiTriTrainDtm = value['dtm']\n",
    "      elif key == 'bigrams':\n",
    "        biSpecs = value['specs']\n",
    "        biTrainDtm = value['dtm']\n",
    "      elif key == 'biTrigrams':\n",
    "        biTriSpecs = value['specs']\n",
    "        biTriTrainDtm = value['dtm']\n",
    "      elif key == 'trigrams':\n",
    "        triSpecs = value['specs']\n",
    "        triTrainDtm = value['dtm']\n",
    "    # checking for file\n",
    "    fileName = self.extractInfo_specs(uniSpecs, 'file_name')\n",
    "    pathTofileName = self.check_file(fileName)\n",
    "    xl = XL_Results_writing(pathTofileName)\n",
    "    # unigrams\n",
    "    df = self.stratified_cv(uniTrainDtm, y, uniSpecs, n_splits=k_fold)\n",
    "    xl.save_resultsToExcel(df, self.extractInfo_specs(uniSpecs, 'sheet_name'),\\\n",
    "                           fileName, self.extractInfo_specs(uniSpecs, 'freq'))\n",
    "    # uniBigrams\n",
    "    df = self.stratified_cv(uniBiTrainDtm, y, uniBiSpecs, n_splits=k_fold)\n",
    "    xl.save_resultsToExcel(df, self.extractInfo_specs(uniBiSpecs, 'sheet_name')\\\n",
    "                           , fileName, self.extractInfo_specs(uniBiSpecs, 'freq'))\n",
    "    # uniBiTrigrams\n",
    "    df = self.stratified_cv(uniBiTriTrainDtm, y, uniBiTriSpecs, n_splits=k_fold)\n",
    "    xl.save_resultsToExcel(df, self.extractInfo_specs(uniBiTriSpecs, 'sheet_name')\\\n",
    "                           , fileName, self.extractInfo_specs(uniBiTriSpecs, 'freq'))\n",
    "    # bigrams\n",
    "    df = self.stratified_cv(biTrainDtm, y, biSpecs, n_splits=k_fold)\n",
    "    xl.save_resultsToExcel(df, self.extractInfo_specs(biSpecs, 'sheet_name')\\\n",
    "                           , fileName, self.extractInfo_specs(biSpecs, 'freq'))\n",
    "    # biTrigrams\n",
    "    df = self.stratified_cv(biTriTrainDtm, y, biTriSpecs, n_splits=k_fold)\n",
    "    xl.save_resultsToExcel(df, self.extractInfo_specs(biTriSpecs, 'sheet_name')\\\n",
    "                           , fileName, self.extractInfo_specs(biTriSpecs, 'freq'))\n",
    "    # trigrams\n",
    "    df = self.stratified_cv(triTrainDtm, y, triSpecs, n_splits=k_fold)\n",
    "    xl.save_resultsToExcel(df, self.extractInfo_specs(triSpecs, 'sheet_name')\\\n",
    "                           , fileName, self.extractInfo_specs(triSpecs, 'freq'))\n",
    "    \n",
    "    xl.generate_resultantWorkSheet('Accuracy', 'Accuracy')\n",
    "#     xl.generate_resultantWorkSheet('Precision', 'Precision')\n",
    "#     xl.generate_resultantWorkSheet('Recall', 'Recall')\n",
    "#     xl.generate_resultantWorkSheet('F1-Measure', 'F1-Measure')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "tc = Training_Classifiers()\n",
    "tc.training_clfs(features, y, k_fold = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uniSpecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-27e7a883c71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniSpecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'uniSpecs' is not defined"
     ]
    }
   ],
   "source": [
    "type(uniSpecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Features:\n",
      "[==================================================] [100.0%] [eta:0:00:00]\n",
      "scritp completed\n",
      "\n",
      "Total time for script completion :0:00:00\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sahibzada/ipynb files/request_identification\")\n",
    "import pandas as pd\n",
    "from utilities.my_progressBar import My_progressBar \n",
    "import scipy\n",
    "import time\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "class Ngrams_Rules_Features:\n",
    "  '''Generating Ngrams and rules features'''\n",
    "  def apply_regex(self, text, regex):\n",
    "    match_found = (re.search(regex ,text) !=None)\n",
    "    match_found = int(match_found == True)\n",
    "    return match_found\n",
    "\n",
    "  def gen_rules_features(self, X_data_series): # , X_data_dtm, features_arg):\n",
    "    '''sparse matrix and series matrices should be converted to dataframe for applying rules and treating\n",
    "    it as features...\n",
    "    I wrote two functions i.e.,sparse_matrix_to_DataFrame() and series_DataFrame()\n",
    "      for changing datatypes'''\n",
    "    X_data_DF = self.series_to_DataFrame(X_data_series)\n",
    "    regexes = [\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(am|are|will be)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I\\'m)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(we\\'re)\\b.*\\b(bringing|giving|helping|raising|donating|auctioning)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(will|would like to)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(we\\'ll)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(ready|prepared)\\b.*\\b(bring|give|help|raise|donate|auction)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(where)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(I|we)\\b.*\\b(like|want)\\b.*\\bto\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(will be)\\b.*\\b(brought|given|raised|donated|auctioned)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b\\w*\\s*\\b\\?', re.I|re.M),\n",
    "      re.compile(r'\\b(you|u).*(can|could|should|want to)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(can|could|should).*(you|u)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(like|want)\\b.*\\bto\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(bring|give|help|raise|donate)\\b', re.I|re.M),\n",
    "      re.compile(r'\\b(how)\\b.*\\b(can I|can we)\\b.*\\b(work|volunteer|assist)\\b', re.I|re.M)\n",
    "\n",
    "    ]\n",
    "    temp = pd.DataFrame()\n",
    "    features_arg = []\n",
    "    for i, regex in zip(range(len(regexes)), regexes):\n",
    "      columnName = \"RegEx_\" + str(i + 1)\n",
    "      features_arg.append(columnName)\n",
    "      temp[columnName] = X_data_DF['tweet-text'].apply(lambda text: self.apply_regex(text, regex))\n",
    "    temp_sparse = scipy.sparse.csr_matrix(temp.values)\n",
    "    return temp_sparse, features_arg\n",
    "\n",
    "  def concat_sparse_matrices_h(self, data_X_dtm, data_Rules_dtm, features_X, features_Rules):\n",
    "    combined_features = features_X + features_Rules\n",
    "    concat_sparse = scipy.sparse.hstack([data_X_dtm, data_Rules_dtm], format='csr')\n",
    "    return concat_sparse, combined_features\n",
    "\n",
    "  def gen_Ngrams(self, X_train, X_test, lower, higher):\n",
    "    Vect = CountVectorizer(ngram_range=(lower, higher))\n",
    "    X_train_dtm = Vect.fit_transform(X_train)\n",
    "    X_test_dtm = Vect.transform(X_test)\n",
    "    return X_train_dtm, X_test_dtm, Vect.get_feature_names()\n",
    "\n",
    "  def sparse_matrix_to_DataFrame(self, X_data_dtm, features):\n",
    "    X_data_dtm = pd.DataFrame(X_data_dtm.toarray(), columns=features)\n",
    "    return X_data_dtm\n",
    "\n",
    "  def series_to_DataFrame(self, X_data):\n",
    "    X_data = X_data.to_frame()\n",
    "    return X_data\n",
    "\n",
    "\n",
    "  def apply_gen_rules_features(self, X, X_train_WoR_dtm, X_train_WoR_Features):\n",
    "    X_Rules_dtm, features_Rules = self.gen_rules_features(X)\n",
    "    X_train_WR_dtm, combined_features = self.concat_sparse_matrices_h(X_train_WoR_dtm, X_Rules_dtm, X_train_WoR_Features, features_Rules)\n",
    "    return  X_train_WR_dtm, combined_features\n",
    "\n",
    "\n",
    "  def simple_CountVect(self, X, lower, higher, minDF, maxDF, include_rules):\n",
    "    Vect = CountVectorizer(ngram_range=(lower, higher), min_df = minDF, max_df = maxDF)\n",
    "    X_train_WoR_dtm = Vect.fit_transform(X)\n",
    "    X_train_Features = Vect.get_feature_names()\n",
    "    if include_rules == 'yes':\n",
    "      X_train_dtm, X_train_Features  = self.apply_gen_rules_features(X, X_train_WoR_dtm, X_train_Features)\n",
    "    else:\n",
    "      X_train_dtm = X_train_WoR_dtm\n",
    "    return X_train_dtm, X_train_Features, X_train_dtm.shape\n",
    "\n",
    "  def tfIdf_Vect(self, X, lower, higher, minDF, maxDF, include_rules):\n",
    "    Vect = TfidfVectorizer(ngram_range=(lower, higher), min_df=minDF, max_df=maxDF)\n",
    "    X_train_WoR_dtm = Vect.fit_transform(X)\n",
    "    X_train_Features = Vect.get_feature_names()\n",
    "    if include_rules == 'yes':\n",
    "      X_train_dtm, X_train_Features  = self.apply_gen_rules_features(X, X_train_WoR_dtm, X_train_Features)\n",
    "    else:\n",
    "      X_train_dtm = X_train_WoR_dtm\n",
    "    return X_train_dtm, X_train_Features, X_train_dtm.shape\n",
    "\n",
    "\n",
    "  def generate_Features(self, X, vect_type = 'simple', minDF = 1, maxDF = 1.0, include_rules = 'yes'):\n",
    "    '''Generating Features from data\n",
    "    -Takes attributes,responses, vect_type(simple by default), minDF, maxDF'''\n",
    "    #generating name for the dataset file\n",
    "    temp_label = ''\n",
    "    if vect_type == 'simple':\n",
    "      temp_label = '_CountVect'\n",
    "      func_handler = self.simple_CountVect\n",
    "    else:\n",
    "      temp_label = '_TfIdfVect'\n",
    "      func_handler = self.tfIdf_Vect\n",
    "    temp_label = temp_label +'_minDF-' + str(minDF) + '_maxDF-' + str(maxDF)\n",
    "    if include_rules == 'yes':\n",
    "      temp_label = temp_label + '_WRul'\n",
    "    else:\n",
    "      temp_label = temp_label + '_WoRul'\n",
    "    pBar = My_progressBar('Generating Features:',6)\n",
    "    for i in range(1, 4):\n",
    "      for j in range(i, 4):\n",
    "        start_time = time.time()\n",
    "        if i == j == 1:\n",
    "          X_train_dtm_uni, X_train_Features_uni, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          uniLabel = \"Unigrams\" + temp_label + '_Freq-' + str(dim[1])\n",
    "        elif (i == 1) & (j == 2):\n",
    "          X_train_dtm_uniBi, X_train_Features_uniBi, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          uniBiLabel = \"UniAndBigrams\"+ temp_label + '_Freq-' + str(dim[1])\n",
    "        elif (i == 1) & (j == 3):\n",
    "          X_train_dtm_uniBiTri, X_train_Features_uniBiTri, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          uniBiTriLabel = \"UniBiAndTrigrams\" + temp_label + '_Freq-' + str(dim[1])\n",
    "        elif (i == 2) & (j == 2):\n",
    "          X_train_dtm_bi, X_train_Features_bi, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          biLabel = \"Bigrams\"+ temp_label + '_Freq-' + str(dim[1])\n",
    "        elif (i == 2) & (j == 3):\n",
    "          X_train_dtm_biTri, X_train_Features_biTri, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          biTriLabel = \"BiAndTri-grams\" + temp_label + '_Freq-' + str(dim[1])\n",
    "        elif (i == 3) & (j == 3):\n",
    "          X_train_dtm_tri, X_train_Features_tri, dim = \\\n",
    "              func_handler(X, i, j, minDF, maxDF, include_rules)\n",
    "          triLabel = \"Trigrams\" + temp_label + '_Freq-' + str(dim[1])\n",
    "        pBar.call_to_progress(start_time)\n",
    "    encap = {'unigrams':{'dtm':X_train_dtm_uni, 'header':X_train_Features_uni, 'specs': uniLabel},\\\n",
    "          'uniBigrams':{'dtm':X_train_dtm_uniBi, 'header':X_train_Features_uniBi, 'specs': uniBiLabel},\\\n",
    "          'uniBiTrigrams':{'dtm':X_train_dtm_uniBiTri, 'header':X_train_Features_uniBiTri, 'specs': uniBiTriLabel},\\\n",
    "          'bigrams':{'dtm':X_train_dtm_bi, 'header':X_train_Features_bi, 'specs': biLabel},\\\n",
    "          'biTrigrams':{'dtm':X_train_dtm_biTri, 'header':X_train_Features_biTri, 'specs': biTriLabel},\\\n",
    "          'trigrams':{'dtm':X_train_dtm_tri, 'header':X_train_Features_tri, 'specs': triLabel}}\n",
    "    return encap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "script_start = time.time() #script start point\n",
    "# fnameTrainingDataset = \"/home/sahibzada/Desktop/ipythonNB/ThImp/Datasets/specific labelled/preprocessed_dataset_specific.csv\"\n",
    "# fnameTrainingDataseet = \"/home/sahibzada/Desktop/ipythonNB/ThImp/Datasets/TestingDatasets/preprocessed_dataset.csv\"\n",
    "fnameTrainingDataseet = \"/home/sahibzada/ipynb files/Datasets/preprocessed_approx100samples.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# dataset['tweet-class'] = dataset['tweet-class'].map({'request': 1, 'not_excl': 0})\n",
    "df = pd.read_csv(fnameTrainingDataseet)\n",
    "X = df['tweet-text']\n",
    "y = df['tweet-class']\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "feat_extract = Ngrams_Rules_Features()\n",
    "features = feat_extract.generate_Features(X,'simple', 1, 1.0, 'no')\n",
    "# print(features['unigrams']['specs'])\n",
    "# print(features['uniBigrams']['specs'])\n",
    "# print(features['uniBiTrigrams']['specs'])\n",
    "# print(features['bigrams']['specs'])\n",
    "# print(features['biTrigrams']['specs'])\n",
    "# print(features['trigrams']['specs'])\n",
    "# mat = features['unigrams']['dtm']\n",
    "# head = features['unigrams']['header']\n",
    "# data = pd.DataFrame(mat.toarray(), columns = head)\n",
    "# print(data)\n",
    "\n",
    "print(\"\\nscritp completed\")\n",
    "Total_time = time.time() - script_start\n",
    "print(\"\\nTotal time for script completion :\" + str(datetime.timedelta(seconds=int(Total_time))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/sahibzada/ipynb files/request_identification\")\n",
    "from utilities.my_progressBar import My_progressBar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d74c320bf614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mextractInfo_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spces'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sheet_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'value' is not defined"
     ]
    }
   ],
   "source": [
    "def extractInfo_specs(st, req):\n",
    "  '''Def: fetch required info from list\n",
    "  Args: pass list and req arguments where req contained name of the required info to be fetched from list\n",
    "  Ret: return required info'''\n",
    "  st = st.split('_')\n",
    "  if req == 'feature_name':\n",
    "    info = st[0]\n",
    "  elif req == 'file_name':\n",
    "    info = self.list_str(st[1:])\n",
    "  elif req == 'freq':\n",
    "    info = st[-1].split('-')[1]\n",
    "  return info\n",
    "extractInfo_specs(value['spces'], 'sheet_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipythonNB",
   "language": "python",
   "name": "ipythonnb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
